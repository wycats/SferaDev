# Language Model Provider Stream Semantics

**Version:** 1.0 Draft  
**Date:** January 30, 2026  
**Status:** Internal Specification  
**Scope:** VS Code Language Model Chat Provider API (`vscode.lm`)

---

## Overview

This document specifies the semantic contract between a Language Model Chat Provider and its consumers (Chat Participants, extensions using `LanguageModelChat.sendRequest()`).

VS Code's API documentation defines the _types_ but not the _behavioral semantics_. This spec fills that gap.

---

## Part I: Stream Fundamentals

### 1.1 Stream Model

A Language Model Provider emits a stream of **parts** via the `Progress<LanguageModelResponsePart>` callback:

```typescript
provideLanguageModelChatResponse(
  model: T,
  messages: readonly LanguageModelChatRequestMessage[],
  options: ProvideLanguageModelChatResponseOptions,
  progress: Progress<LanguageModelResponsePart>,  // ← The stream
  token: CancellationToken
): Thenable<void>
```

**Stream characteristics:**

| Property         | Guarantee | Notes                                                                  |
| ---------------- | --------- | ---------------------------------------------------------------------- |
| **Ordering**     | FIFO      | Parts MUST arrive in the order they were generated by the model        |
| **Atomicity**    | Per-part  | Each `progress.report(part)` is atomic; partial parts are not allowed  |
| **Completion**   | Implicit  | Stream ends when the `Thenable<void>` resolves                         |
| **Cancellation** | Via token | Provider MUST stop emitting parts when `token.isCancellationRequested` |

### 1.2 Part Types

The `LanguageModelResponsePart` union includes:

```typescript
type LanguageModelResponsePart =
  | LanguageModelTextPart
  | LanguageModelToolCallPart
  | LanguageModelToolResultPart // Note: Only valid in messages, not provider responses
  | LanguageModelDataPart;
```

**Important:** `LanguageModelToolResultPart` is listed in the union but MUST NOT be emitted by providers. It is only valid in message content (representing a tool result from a previous turn).

---

## Part II: Text Parts

### 2.1 Definition

```typescript
class LanguageModelTextPart {
  constructor(value: string);
  value: string;
}
```

### 2.2 Semantics

| Aspect            | Specification                                                                       |
| ----------------- | ----------------------------------------------------------------------------------- |
| **Content**       | UTF-8 text, may contain markdown, code blocks, etc.                                 |
| **Visibility**    | SHOULD be displayed to the user as chat output                                      |
| **Streaming**     | MAY be emitted incrementally (token-by-token or chunk-by-chunk)                     |
| **Concatenation** | Consumer MUST concatenate consecutive TextParts into a single logical text response |
| **Interleaving**  | MAY be interleaved with other part types                                            |

### 2.3 Streaming Text

Providers typically emit text incrementally:

```typescript
// Model generates: "Hello, world!"
progress.report(new LanguageModelTextPart("Hello"));
progress.report(new LanguageModelTextPart(", "));
progress.report(new LanguageModelTextPart("world!"));
```

**Consumer expectation:** Display each chunk as it arrives for low-latency UX.

### 2.4 Thinking Content (Proposed API)

VS Code has a **proposed** `LanguageModelThinkingPart` for streaming reasoning content from extended thinking models. When enabled, the stream becomes:

```typescript
// Proposed API stream (requires enabledApiProposals: ["languageModelThinkingPart"])
stream: AsyncIterable<
  | LanguageModelTextPart
  | LanguageModelThinkingPart
  | LanguageModelToolCallPart
  | unknown
>;
```

**ThinkingPart definition:**

```typescript
class LanguageModelThinkingPart {
  value: string | string[]; // Thinking content (single chunk or array)
  id?: string; // Optional identifier for the thinking block
  metadata?: { readonly [key: string]: any }; // Optional metadata
}
```

**Current status (January 2026):**

| Aspect                  | Status                                                                    |
| ----------------------- | ------------------------------------------------------------------------- |
| **Proposed API**        | `languageModelThinkingPart` - merged August 2025                          |
| **First-party support** | Copilot uses this internally                                              |
| **Third-party support** | Published extensions cannot use proposed APIs                             |
| **UI rendering**        | Chat Participants automatically render ThinkingPart as collapsible blocks |

**Why thinking "just works" for Copilot:**

1. Copilot's internal LM provider emits `LanguageModelThinkingPart`
2. Chat Participants receive it via the `stream` property on `LanguageModelChatResponse`
3. VS Code's chat UI renders ThinkingPart as collapsed reasoning blocks
4. **No parsing required** - it's a first-class type, not text with markers

**For third-party providers (like AI Gateway):**

Until the API is stable, providers have two options:

1. **Emit thinking as text** - Visible to user, no special treatment
2. **Suppress thinking** - Not emitted at all

See [GitHub issue #246993](https://github.com/microsoft/vscode/issues/246993) for tracking.

### 2.5 "Working..." Blocks (Tool Grouping)

**Important clarification:** The collapsible "Working..." blocks visible in VS Code's chat UI are **NOT** from `LanguageModelThinkingPart`. They are created by **tool invocation grouping**.

When Copilot or a Chat Participant invokes tools, VS Code automatically creates a `ChatThinkingContentPart` to group those tool invocations:

```typescript
// From chatListRenderer.ts (line 1758-1767)
if (!lastThinking && toolInvocation.presentation !== 'hidden' && ...) {
    const thinkingPart = this.renderThinkingPart({
        kind: 'thinking',  // Creates thinking part with EMPTY content
    }, context, templateData);
}
```

**Key insight:** The same UI component (`ChatThinkingContentPart`) is reused for both:

1. **Thinking content** - From `LanguageModelThinkingPart` (reasoning tokens)
2. **Tool grouping** - From tool invocations (automatic grouping)

This means "Working..." blocks appear when tools are invoked, regardless of whether the provider emits ThinkingPart.

**Implication for AI Gateway:**

- Our current behavior of NOT emitting ThinkingPart still results in "Working..." blocks when Copilot invokes tools
- To show model reasoning (e.g., from o1, Claude extended thinking), we would need to emit ThinkingPart
- Alternatively, we could emit synthetic tool calls to trigger the grouping UI (though this is a hack)

### 2.6 Markdown Rendering in TextPart

VS Code's chat UI interprets `LanguageModelTextPart.value` as **GitHub Flavored Markdown (GFM)**. Understanding supported features is critical for proper content rendering.

**Supported Features:**

| Feature       | Syntax                    | Notes                                                         |
| ------------- | ------------------------- | ------------------------------------------------------------- |
| Links         | `[text](url)`             | Clickable, opens externally                                   |
| Bold          | `**text**`                |                                                               |
| Italic        | `*text*` or `_text_`      |                                                               |
| Strikethrough | `~~text~~`                |                                                               |
| Code blocks   | ` ``` `                   | **Full editor widget** with syntax highlighting, hover, links |
| Inline code   | `` `code` ``              |                                                               |
| Lists         | `- item` or `1. item`     | Ordered and unordered                                         |
| Tables        | GFM table syntax          |                                                               |
| Blockquotes   | `> text`                  | Including GitHub alert syntax                                 |
| Math (KaTeX)  | `$inline$` or `$$block$$` | When config enabled                                           |
| Theme icons   | `$(icon-name)`            | Codicon syntax                                                |

**Restricted Features:**

| Feature          | Status          | Notes                                             |
| ---------------- | --------------- | ------------------------------------------------- |
| External images  | ❌ **Blocked**  | `![](url)` will not render                        |
| Raw HTML         | ❌ Stripped     | Unless `supportHtml` enabled (not for LM content) |
| `command:` links | ⚠️ Trusted only | Requires explicit trust                           |
| Checkboxes       | ⚠️ Disabled     | Render but not interactive                        |

**Critical Implication:** Do NOT attempt to render external images via markdown. Use `LanguageModelDataPart` for image content, though note that DataPart is also not rendered by default in chat UI.

**Special URI Schemes (Chat Participants only):**

- `http://_vscodecontentref_` → File reference widgets
- `http://_vscodedecoration_` → Decoration widgets
- `http://_chatagent_` → Agent references

**Provenance:**

- Renderer: [chatContentMarkdownRenderer.ts](https://github.com/microsoft/vscode/blob/main/src/vs/workbench/contrib/chat/browser/widget/chatContentMarkdownRenderer.ts)
- Sanitization: [markdownRenderer.ts](https://github.com/microsoft/vscode/blob/main/src/vs/base/browser/markdownRenderer.ts)
- Code blocks: [codeBlockPart.ts](https://github.com/microsoft/vscode/blob/main/src/vs/workbench/contrib/chat/browser/widget/chatContentParts/codeBlockPart.ts)

---

## Part III: Tool Call Parts

### 3.1 Definition

```typescript
class LanguageModelToolCallPart {
  constructor(callId: string, name: string, input: object);
  callId: string; // Unique identifier for this invocation
  name: string; // Tool name (must match a tool in request options)
  input: object; // Tool arguments (JSON-serializable)
}
```

### 3.2 Semantics

| Aspect             | Specification                                                    |
| ------------------ | ---------------------------------------------------------------- |
| **Atomicity**      | Tool call parts are ATOMIC - no incremental argument streaming   |
| **Uniqueness**     | `callId` MUST be unique within the response stream               |
| **Completeness**   | When emitted, the tool call is COMPLETE and ready for invocation |
| **Multiple Calls** | A single response MAY contain multiple tool calls                |
| **Ordering**       | Multiple tool calls MAY be emitted in any order                  |

### 3.3 CallId Requirements

The `callId` serves as a correlation key between the tool call and its result:

```typescript
// Provider emits
progress.report(new LanguageModelToolCallPart(
  "call_abc123",           // callId - must be unique
  "read_file",             // name
  { path: "/src/app.ts" }  // input
));

// Consumer sends result back (in next turn's messages)
new LanguageModelToolResultPart(
  "call_abc123",           // Must match the callId
  [...]                    // Result content
)
```

**Namespacing recommendation:** To avoid collisions when multiple providers are active, prefix callIds with a provider-specific namespace (e.g., `gw-call_abc123`).

### 3.4 Tool Call Emission Timing

Tool calls SHOULD be emitted as soon as the model has fully specified the call:

```
Model stream:    "Let me check" → [tool_call] → "Based on the file..."
Provider emits:  TextPart       → ToolCallPart → TextPart
```

**Not allowed:** Emitting a ToolCallPart before the arguments are complete.

### 3.5 Parallel Tool Calls

When a model requests multiple tools in parallel:

```typescript
// Both calls emitted before any text continues
progress.report(new LanguageModelToolCallPart("call_1", "read_file", {...}));
progress.report(new LanguageModelToolCallPart("call_2", "list_dir", {...}));
// Consumer executes both, sends both results, continues conversation
```

---

## Part IV: Data Parts

### 4.1 Definition

```typescript
class LanguageModelDataPart {
  static image(data: Uint8Array, mime: string): LanguageModelDataPart;
  static json(value: any, mime?: string): LanguageModelDataPart;
  static text(value: string, mime?: string): LanguageModelDataPart;

  constructor(data: Uint8Array, mimeType: string);
  data: Uint8Array;
  mimeType: string;
}
```

### 4.2 Semantics

| Aspect                | Specification                            |
| --------------------- | ---------------------------------------- |
| **Content**           | Arbitrary binary or structured data      |
| **Visibility**        | NOT automatically rendered in chat UI    |
| **Use Cases**         | Images, files, structured outputs (JSON) |
| **Consumer Handling** | Consumer decides whether/how to display  |

### 4.3 Rendering Behavior

**Important:** `LanguageModelDataPart` is NOT rendered in VS Code's chat UI by default. It is intended for:

1. **Programmatic consumption** - Extensions that need structured data
2. **Image output** - When the model generates images
3. **File artifacts** - Generated files, diagrams, etc.

If a provider wants content visible in chat, it MUST use `LanguageModelTextPart` (with markdown for images/links).

---

## Part V: Stream Lifecycle

### 5.1 Normal Completion

```
┌─────────────────────────────────────────────────────────┐
│  provideLanguageModelChatResponse() called              │
│                    │                                    │
│                    ▼                                    │
│  ┌─────────────────────────────────────────────────┐    │
│  │           Streaming Phase                       │    │
│  │                                                 │    │
│  │  progress.report(TextPart)                      │    │
│  │  progress.report(TextPart)                      │    │
│  │  progress.report(ToolCallPart)                  │    │
│  │  progress.report(TextPart)                      │    │
│  │  ...                                            │    │
│  └─────────────────────────────────────────────────┘    │
│                    │                                    │
│                    ▼                                    │
│  Promise resolves (void) ─────────────────────────────  │
│  Stream is COMPLETE                                     │
└─────────────────────────────────────────────────────────┘
```

### 5.2 Cancellation

When `token.isCancellationRequested` becomes true:

1. Provider SHOULD stop requesting data from upstream
2. Provider MAY emit a final partial TextPart with content received so far
3. Provider SHOULD resolve the promise (not reject)
4. Consumer SHOULD treat partial output as valid

### 5.3 Error Handling

If the provider encounters an error:

1. Provider SHOULD reject the promise with a `LanguageModelError`
2. Consumer displays error to user
3. Any parts emitted before the error are still valid

```typescript
if (apiError) {
  throw new vscode.LanguageModelError(
    "API rate limit exceeded",
    vscode.LanguageModelError.Blocked.name, // or appropriate error type
  );
}
```

**Implementation Note (AI Gateway):**

The AI Gateway currently emits errors as `LanguageModelTextPart` rather than throwing `LanguageModelError`. This is a **workaround** to avoid VS Code's "no response returned" error when no parts have been emitted.

```typescript
// Current behavior (workaround)
progress.report(
  new LanguageModelTextPart(`\n\n**Error:** ${errorMessage}\n\n`),
);
```

**Rationale:** When VS Code's `provideLanguageModelChatResponse` rejects without emitting any parts, VS Code displays a generic "no response returned" error. Emitting the error as text ensures users see the actual error message.

**Future improvement:** If VS Code's behavior allows rejecting with `LanguageModelError` while preserving partial output, switch to proper error rejection.

---

## Part VI: Interleaving and Ordering

### 6.1 Allowed Interleaving Patterns

Parts MAY interleave in any order, subject to these constraints:

```
✅ Allowed:
TextPart → TextPart → ToolCallPart → TextPart
TextPart → ToolCallPart → ToolCallPart → TextPart
ToolCallPart → TextPart → ToolCallPart → TextPart

❌ Not Allowed:
ToolCallPart (partial) → ToolCallPart (rest)  // Must be atomic
ToolResultPart (in response stream)            // Only valid in messages
```

### 6.2 Text Continuity

Consecutive TextParts are logically continuous:

```typescript
progress.report(new LanguageModelTextPart("Hello "));
progress.report(new LanguageModelTextPart("world"));
// Consumer sees: "Hello world" (one logical text block)
```

### 6.3 Interleaved Text and Tool Calls

When text and tool calls interleave, the text represents the model's reasoning:

```typescript
progress.report(new LanguageModelTextPart("Let me check that file."));
progress.report(new LanguageModelToolCallPart("call_1", "read_file", {...}));
progress.report(new LanguageModelTextPart("Based on the contents..."));
```

**Consumer expectation:**

1. Display "Let me check that file."
2. Show tool call indicator
3. Execute tool and wait for result
4. Send result back to model in next turn
5. Display continued response

---

## Part VII: Consumer Responsibilities

### 7.1 Required Behaviors

| Responsibility          | Implementation                                 |
| ----------------------- | ---------------------------------------------- |
| **Concatenate text**    | Join consecutive TextParts into logical blocks |
| **Execute tool calls**  | Invoke tools and provide results back          |
| **Handle cancellation** | Stop processing when cancelled                 |
| **Display errors**      | Show LanguageModelError to user                |

### 7.2 Optional Behaviors

| Behavior              | Notes                                   |
| --------------------- | --------------------------------------- |
| **Render data parts** | Consumer decides if/how to display      |
| **Show progress**     | Consumer may show "thinking" indicators |
| **Buffer output**     | Consumer may buffer before displaying   |

### 7.3 Multi-Turn Tool Call Flow

When a tool call is emitted, the consumer MUST:

1. Execute the tool
2. Send a new request with:
   - Original messages
   - An assistant message containing the ToolCallPart
   - A user message containing the ToolResultPart with matching callId
3. Continue streaming the response

---

## Part VIII: Provider Responsibilities

### 8.1 Required Behaviors

| Responsibility            | Implementation                                 |
| ------------------------- | ---------------------------------------------- |
| **Unique callIds**        | Generate unique, stable callIds for tool calls |
| **Atomic tool calls**     | Emit complete ToolCallParts only               |
| **Ordered emission**      | Emit parts in generation order                 |
| **Respect cancellation**  | Stop emitting when token is cancelled          |
| **Resolve on completion** | Resolve promise when stream ends               |

### 8.2 Recommended Behaviors

| Behavior               | Notes                                 |
| ---------------------- | ------------------------------------- |
| **Namespace callIds**  | Prefix with provider identifier       |
| **Emit incrementally** | Stream text as received, don't buffer |
| **Log errors**         | Log upstream errors for debugging     |

---

## Part IX: Proposed API Summary

This section summarizes VS Code's proposed Language Model APIs relevant to providers. For full details, see [RFC 019: Proposed LM Provider APIs](../rfcs/stage-0/019-proposed-lm-provider-apis.md).

### 9.1 API Overview

| API                               | Relevance                                       | Status                    |
| --------------------------------- | ----------------------------------------------- | ------------------------- |
| `chatProvider`                    | **Core** - Provider registration                | v4, active development    |
| `languageModelThinkingPart`       | **High** - Reasoning tokens                     | v1, stabilizing           |
| `languageModelCapabilities`       | **Medium** - Capability exposure                | Stable shape              |
| `languageModelSystem`             | **High** - System messages                      | Stalled, uncertain design |
| `languageModelToolResultAudience` | **Medium** - Content routing                    | Recently generalized      |
| `toolProgress`                    | **Low** - Tool progress (for tool implementers) | Simple, likely stable     |

### 9.2 Marketplace Constraints

**⚠️ Proposed APIs cannot be used in published Marketplace extensions.**

For development/testing with Insiders:

```json
// package.json
{
  "enabledApiProposals": ["chatProvider", "languageModelThinkingPart"]
}
```

Run with: `code-insiders --enable-proposed-api <extension-id>`

### 9.3 Feature Detection Pattern

```typescript
function hasThinkingPartSupport(): boolean {
  return typeof vscode.LanguageModelThinkingPart !== "undefined";
}

// Usage with fallback
if (hasThinkingPartSupport()) {
  progress.report(new vscode.LanguageModelThinkingPart(thinkingContent));
} else {
  progress.report(new vscode.LanguageModelTextPart(thinkingContent));
}
```

### 9.4 Extended Part Types (Proposed)

When `languageModelThinkingPart` is available:

```typescript
type LanguageModelResponsePart2 =
  | LanguageModelResponsePart // Stable types
  | LanguageModelDataPart
  | LanguageModelThinkingPart; // Proposed

class LanguageModelThinkingPart {
  value: string | string[];
  id?: string;
  metadata?: { readonly [key: string]: any };
}
```

### 9.5 Current Stream Adapter Behavior

The AI Gateway's `stream-adapter.ts` currently handles OpenResponses reasoning events but emits them as `LanguageModelTextPart`:

| OpenResponses Event                | Current Emission        | Ideal Emission (Proposed)   |
| ---------------------------------- | ----------------------- | --------------------------- |
| `response.reasoning.delta`         | `LanguageModelTextPart` | `LanguageModelThinkingPart` |
| `response.reasoning.done`          | `LanguageModelTextPart` | `LanguageModelThinkingPart` |
| `response.reasoning_summary.delta` | `LanguageModelTextPart` | `LanguageModelThinkingPart` |

When the proposed API becomes stable, the adapter should switch to emitting `LanguageModelThinkingPart` for reasoning content.

---

## Appendix A: Mapping to OpenResponses SSE

For providers translating OpenResponses streaming to VS Code:

| OpenResponses Event                     | VS Code Part      | Notes                                      |
| --------------------------------------- | ----------------- | ------------------------------------------ |
| `response.output_text.delta`            | TextPart          | Concatenate deltas                         |
| `response.function_call_arguments.done` | ToolCallPart      | Emit atomically                            |
| `response.reasoning.delta`              | TextPart\*        | \*ThinkingPart when proposed API available |
| `response.reasoning.done`               | TextPart\*        | \*ThinkingPart when proposed API available |
| `response.reasoning_summary.delta`      | TextPart\*        | \*ThinkingPart when proposed API available |
| `response.reasoning_summary.done`       | TextPart\*        | \*ThinkingPart when proposed API available |
| `response.completed`                    | (resolve promise) | End of stream                              |

**Event naming note:** The OpenResponses API uses underscore-separated event names (e.g., `reasoning_summary`) rather than dot-separated (e.g., `reasoning.summary`). The implementation follows the OpenAPI schema's enum values.

**Note on `response.file`:** This event type is mentioned in some documentation but does not currently exist in the OpenResponses OpenAPI schema. When/if it becomes available, map to `LanguageModelDataPart`.

**Note on reasoning:** Reasoning events currently map to `LanguageModelTextPart` because `LanguageModelThinkingPart` is a proposed API. When the API becomes stable, switch to `ThinkingPart` for proper UI treatment (collapsible reasoning blocks).

---

## Appendix B: Known Gaps and Future API

### Proposed APIs (Cannot Use in Marketplace Extensions)

For complete documentation of all proposed APIs, see [RFC 019](../rfcs/stage-0/019-proposed-lm-provider-apis.md).

| API                               | Purpose                                | Tracking                                                                                                                                                                  |
| --------------------------------- | -------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `languageModelThinkingPart`       | Reasoning/thinking content             | [vscode.proposed.languageModelThinkingPart.d.ts](https://github.com/microsoft/vscode/blob/main/src/vscode-dts/vscode.proposed.languageModelThinkingPart.d.ts)             |
| `languageModelCapabilities`       | Consumer-queryable capabilities        | [vscode.proposed.languageModelCapabilities.d.ts](https://github.com/microsoft/vscode/blob/main/src/vscode-dts/vscode.proposed.languageModelCapabilities.d.ts)             |
| `languageModelSystem`             | System message role                    | [#206265](https://github.com/microsoft/vscode/issues/206265)                                                                                                              |
| `languageModelToolResultAudience` | Content routing (user/model/extension) | [vscode.proposed.languageModelToolResultAudience.d.ts](https://github.com/microsoft/vscode/blob/main/src/vscode-dts/vscode.proposed.languageModelToolResultAudience.d.ts) |

### Current Workarounds

| Gap                    | Workaround                                                                |
| ---------------------- | ------------------------------------------------------------------------- |
| **Thinking content**   | Emit as `LanguageModelTextPart` (visible but not collapsible) or suppress |
| **System messages**    | Map to OpenResponses `developer` role                                     |
| **Capability queries** | Expose via model tags at registration time                                |
| **Content routing**    | All content sent to both model and user                                   |

### Not Yet Proposed

- **Streaming tool arguments** - Incremental tool call emission (would enable progressive UI)
- **Reasoning token metadata** - Structured metadata for reasoning token counts

---

## Appendix C: Glossary

| Term         | Definition                                                    |
| ------------ | ------------------------------------------------------------- |
| **Part**     | A single unit in the response stream                          |
| **CallId**   | Unique identifier correlating tool call and result            |
| **Consumer** | Code that calls `sendRequest()` (Chat Participant, extension) |
| **Provider** | Extension implementing `LanguageModelChatProvider`            |

---

_This specification reflects the VS Code API as of January 2026. Check for updates as the API evolves._
